{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f698391",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:26.201951Z",
     "iopub.status.busy": "2023-08-10T00:04:26.201159Z",
     "iopub.status.idle": "2023-08-10T00:04:26.219208Z",
     "shell.execute_reply": "2023-08-10T00:04:26.218384Z"
    },
    "papermill": {
     "duration": 0.024729,
     "end_time": "2023-08-10T00:04:26.221330",
     "exception": false,
     "start_time": "2023-08-10T00:04:26.196601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tiny-gpt-corpus/Tiny_data.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebd2ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:26.229408Z",
     "iopub.status.busy": "2023-08-10T00:04:26.228602Z",
     "iopub.status.idle": "2023-08-10T00:04:32.240445Z",
     "shell.execute_reply": "2023-08-10T00:04:32.239411Z"
    },
    "papermill": {
     "duration": 6.018713,
     "end_time": "2023-08-10T00:04:32.243294",
     "exception": false,
     "start_time": "2023-08-10T00:04:26.224581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /kaggle/input/tiny-gpt-corpus/Tiny_data.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /kaggle/input/tiny-gpt-corpus/Tiny_data.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9996"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 25670\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24133 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6114 size=20 all=1626 active=1566 piece=it\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3906 size=40 all=2197 active=2137 piece=st\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2392 size=60 all=2967 active=2907 piece=ld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1864 size=80 all=3580 active=3520 piece=ke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1429 size=100 all=4127 active=4067 piece=▁his\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1396 min_freq=99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1253 size=120 all=4692 active=1564 piece=al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=990 size=140 all=5317 active=2189 piece=ess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=828 size=160 all=5831 active=2703 piece=▁To\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=717 size=180 all=6251 active=3123 piece=▁are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=621 size=200 all=6770 active=3642 piece=▁up\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=614 min_freq=90\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=567 size=220 all=7144 active=1366 piece=▁con\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=510 size=240 all=7562 active=1784 piece=▁was\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=478 size=260 all=7868 active=2090 piece=ind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=280 all=8231 active=2453 piece=▁br\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=395 size=300 all=8612 active=2834 piece=fore\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=394 min_freq=80\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=359 size=320 all=8903 active=1286 piece=ak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=334 size=340 all=9181 active=1564 piece=▁had\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=360 all=9480 active=1863 piece=▁speak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=286 size=380 all=9750 active=2133 piece=▁heart\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=270 size=400 all=9971 active=2354 piece=▁In\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=270 min_freq=68\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=255 size=420 all=10185 active=1203 piece=oy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=237 size=440 all=10377 active=1395 piece=▁ro\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=231 size=460 all=10485 active=1503 piece=▁This\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=222 size=480 all=10766 active=1784 piece=urse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=211 size=500 all=10872 active=1890 piece=UM\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=210 min_freq=61\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=520 all=11026 active=1147 piece=ber\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=190 size=540 all=11288 active=1409 piece=uke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=560 all=11492 active=1613 piece=▁true\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=580 all=11614 active=1735 piece=▁doth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=165 size=600 all=11743 active=1864 piece=▁Ed\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=165 min_freq=55\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=162 size=620 all=11879 active=1134 piece=▁MENEN\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=640 all=11983 active=1238 piece=▁PETRUCHIO\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=150 size=660 all=12236 active=1491 piece=ilt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=680 all=12347 active=1602 piece=less\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=700 all=12558 active=1813 piece=▁friends\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=138 min_freq=50\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=720 all=12673 active=1115 piece=▁rest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=740 all=12799 active=1241 piece=▁ISAB\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=760 all=12972 active=1414 piece=▁better\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=780 all=13079 active=1521 piece=ied\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=800 all=13279 active=1721 piece=▁acc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=117 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=820 all=13398 active=1107 piece=▁On\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=840 all=13558 active=1267 piece=ale\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=860 all=13742 active=1451 piece=▁unto\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=880 all=13858 active=1567 piece=▁words\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=900 all=13904 active=1613 piece=ty\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=103 min_freq=41\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=920 all=14065 active=1147 piece=▁Good\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=940 all=14185 active=1267 piece=▁All\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=95 size=960 all=14302 active=1384 piece=APUL\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=980 all=14357 active=1439 piece=bt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=1000 all=14439 active=1521 piece=▁From\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=90 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=1020 all=14553 active=1115 piece=▁Our\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=1040 all=14637 active=1199 piece=GARET\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83 size=1060 all=14709 active=1271 piece=GELO\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=1080 all=14776 active=1338 piece=▁ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=1100 all=14827 active=1389 piece=▁maid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=80 min_freq=34\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=1120 all=14887 active=1058 piece=▁Al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=1140 all=15101 active=1272 piece=▁DUCH\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=1160 all=15185 active=1356 piece=▁att\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=1180 all=15266 active=1437 piece=▁vo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=1200 all=15332 active=1503 piece=▁even\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=70 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=1220 all=15366 active=1034 piece=ury\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=1240 all=15451 active=1119 piece=ouch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=1260 all=15527 active=1195 piece=▁lies\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=1280 all=15590 active=1258 piece=ENV\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=1300 all=15637 active=1305 piece=▁holy\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=63 min_freq=30\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=1320 all=15702 active=1066 piece=▁dare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=1340 all=15728 active=1092 piece=▁Servant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=1360 all=15780 active=1144 piece=▁sovereign\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=1380 all=15860 active=1224 piece=ery\n",
      "bpe_model_trainer.cc(268) LOG(INFO"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "with open('/kaggle/input/tiny-gpt-corpus/Tiny_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "import sentencepiece as spm\n",
    "# 从TXT文件训练BPE模型\n",
    "spm.SentencePieceTrainer.train(input='/kaggle/input/tiny-gpt-corpus/Tiny_data.txt', model_prefix='bpe_model', vocab_size=10000, model_type='bpe')\n",
    "## load the BPE embedding model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('bpe_model.model')\n",
    "\n",
    "#get the vocab ids \n",
    "token_ids = sp.EncodeAsIds(text)\n",
    "\n",
    "# test\n",
    "print(sp.DecodeIds(token_ids)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0716f32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.252185Z",
     "iopub.status.busy": "2023-08-10T00:04:32.251652Z",
     "iopub.status.idle": "2023-08-10T00:04:32.260919Z",
     "shell.execute_reply": "2023-08-10T00:04:32.260004Z"
    },
    "papermill": {
     "duration": 0.016143,
     "end_time": "2023-08-10T00:04:32.263165",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.247022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTconfig:\n",
    "    #define dropout rate for each part of the GPT\n",
    "    embed_dropout=0.1\n",
    "    att_dropout=0.1\n",
    "    ff_dropout=0.1\n",
    "    def __init__(self,vocab_size,max_len,**kwargs):\n",
    "        self.vocab=vocab_size\n",
    "        self.max_len=max_len\n",
    "        for k,v in kwargs.items():\n",
    "            setatter(self,k,v)\n",
    "class GPT2_config(GPTconfig):\n",
    "    num_heads=12\n",
    "    num_blocks=12\n",
    "    embed_dim=768\n",
    "    vocab_size = 10000\n",
    "vocab_size = 10000\n",
    "max_len = 32\n",
    "config = GPT2_config(vocab_size, max_len)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f72cc77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.271963Z",
     "iopub.status.busy": "2023-08-10T00:04:32.271697Z",
     "iopub.status.idle": "2023-08-10T00:04:32.281114Z",
     "shell.execute_reply": "2023-08-10T00:04:32.280239Z"
    },
    "papermill": {
     "duration": 0.015931,
     "end_time": "2023-08-10T00:04:32.283094",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.267163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT_2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        embed_dim=config.embed_dim\n",
    "        self.max_len=config.max_len\n",
    "        \n",
    "        self.token_embed=nn.Embedding(config.vocab_size,embed_dim)\n",
    "        self.pos_embed=nn.Parameter(torch.zeros(1,config.max_len,config.embed_dim))## position embedding ?????\n",
    "        self.dropout=nn.Dropout(config.embed_dropout)\n",
    "        self.blocks=nn.Sequential(*[Transformer_block(config) for i in range(config.num_blocks)])\n",
    "        self.ln=nn.LayerNorm(embed_dim)\n",
    "        self.fc=nn.Linear(embed_dim,config.vocab_size)\n",
    "    def forward(self,x,targets=None):\n",
    "        input_len=x.size(1)\n",
    "        assert input_len<=self.max_len,\"seq longer than the maximum length\"\n",
    "        tok_embedding=self.token_embed(x)\n",
    "        pos_embedding=self.pos_embed[:,:input_len,:]\n",
    "        x=tok_embedding+pos_embedding\n",
    "        x=self.dropout(x)\n",
    "        x=self.blocks(x)\n",
    "        x=self.ln(x)\n",
    "        logits=self.fc(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09256cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.291579Z",
     "iopub.status.busy": "2023-08-10T00:04:32.291322Z",
     "iopub.status.idle": "2023-08-10T00:04:32.297642Z",
     "shell.execute_reply": "2023-08-10T00:04:32.296784Z"
    },
    "papermill": {
     "duration": 0.012963,
     "end_time": "2023-08-10T00:04:32.299680",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.286717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        embed_dim=config.embed_dim\n",
    "        self.ln_for_att=nn.LayerNorm(embed_dim)\n",
    "        self.ln_for_ff=nn.LayerNorm(embed_dim)\n",
    "        self.att=MultiHead_Attention(config)\n",
    "        self.ff=nn.Sequential(\n",
    "            nn.Linear(embed_dim,embed_dim*4),\n",
    "            nn.GELU(),#### nice actiavte function for the transformer achitecture\n",
    "            nn.Linear(embed_dim*4,embed_dim),\n",
    "            nn.Dropout(config.ff_dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.att(self.ln_for_att(x))\n",
    "        x=self.ff(self.ln_for_ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969a871c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.308383Z",
     "iopub.status.busy": "2023-08-10T00:04:32.308125Z",
     "iopub.status.idle": "2023-08-10T00:04:32.317335Z",
     "shell.execute_reply": "2023-08-10T00:04:32.316478Z"
    },
    "papermill": {
     "duration": 0.016011,
     "end_time": "2023-08-10T00:04:32.319363",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.303352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Self_attention(nn.Module):\n",
    "    def __init__(self,config,head_size):\n",
    "        super().__init__()\n",
    "        embed_dim=config.embed_dim\n",
    "        block_size=config.max_len\n",
    "        self.key=nn.Linear(embed_dim,head_size)\n",
    "        self.value=nn.Linear(embed_dim,head_size)\n",
    "        self.query=nn.Linear(embed_dim,head_size)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout=nn.Dropout(config.att_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C=x.shape# Batch,Time step(token),Dimension of each token vector\n",
    "        k=self.key(x)\n",
    "        q=self.query(x)\n",
    "        v = self.value(x)\n",
    "        w=q@k.transpose(-2,-1) # transpose the last 2 dimension, bacase 1 is batch,but 2 and 3 is the matrix for tokens: Q*K'\n",
    "        w=w*C**-0.5 # scaled\n",
    "        w=w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) ## using the trill to mask the token after t token.\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        out = w @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e4f9f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.328304Z",
     "iopub.status.busy": "2023-08-10T00:04:32.327686Z",
     "iopub.status.idle": "2023-08-10T00:04:32.335111Z",
     "shell.execute_reply": "2023-08-10T00:04:32.334271Z"
    },
    "papermill": {
     "duration": 0.013884,
     "end_time": "2023-08-10T00:04:32.336951",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.323067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHead_Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        head_size=config.embed_dim//config.num_heads\n",
    "        num_heads=config.num_heads\n",
    "        embed_dim=config.embed_dim\n",
    "        self.heads=nn.ModuleList([Self_attention(config,head_size) for _ in range(num_heads)])\n",
    "        self.proj=nn.Linear(embed_dim,embed_dim)\n",
    "        self.dropout=nn.Dropout(config.att_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        out=self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483e4d7",
   "metadata": {
    "papermill": {
     "duration": 0.003582,
     "end_time": "2023-08-10T00:04:32.344457",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.340875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bb824f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.353401Z",
     "iopub.status.busy": "2023-08-10T00:04:32.352787Z",
     "iopub.status.idle": "2023-08-10T00:04:32.420850Z",
     "shell.execute_reply": "2023-08-10T00:04:32.419897Z"
    },
    "papermill": {
     "duration": 0.074675,
     "end_time": "2023-08-10T00:04:32.422926",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.348251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "# block_size = 32 # what is the maximum context length for predictions?\n",
    "# max_iters = 5000\n",
    "# eval_interval = 100\n",
    "# learning_rate = 1e-3\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 64\n",
    "# n_head = 4\n",
    "# n_layer = 4\n",
    "# dropout = 0.0\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"batch_size\": 16, # how many independent sequences will we process in parallel?\n",
    "    \"block_size\": 32, # what is the maximum context length for predictions?\n",
    "    \"max_iters\": 5000,\n",
    "    \"eval_interval\": 100,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"eval_iters\": 200,\n",
    "    \"n_embd\": 64,\n",
    "    \"n_head\": 4,\n",
    "    \"n_layer\": 4,\n",
    "    \"dropout\": 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e72a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T00:04:32.432348Z",
     "iopub.status.busy": "2023-08-10T00:04:32.431754Z",
     "iopub.status.idle": "2023-08-10T00:27:19.636570Z",
     "shell.execute_reply": "2023-08-10T00:27:19.635576Z"
    },
    "papermill": {
     "duration": 1367.212705,
     "end_time": "2023-08-10T00:27:19.639586",
     "exception": false,
     "start_time": "2023-08-10T00:04:32.426881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.450576 M parameters\n",
      "step 0: train loss 9.4350\n",
      "step 100: train loss 6.7857\n",
      "step 200: train loss 6.6965\n",
      "step 300: train loss 6.6773\n",
      "step 400: train loss 6.6275\n",
      "step 500: train loss 6.6190\n",
      "step 600: train loss 6.6343\n",
      "step 700: train loss 6.6059\n",
      "step 800: train loss 6.5875\n",
      "step 900: train loss 6.5868\n",
      "step 1000: train loss 6.5991\n",
      "step 1100: train loss 6.5985\n",
      "step 1200: train loss 6.5973\n",
      "step 1300: train loss 6.6189\n",
      "step 1400: train loss 6.6048\n",
      "step 1500: train loss 6.6009\n",
      "step 1600: train loss 6.5850\n",
      "step 1700: train loss 6.5856\n",
      "step 1800: train loss 6.6001\n",
      "step 1900: train loss 6.6076\n",
      "step 2000: train loss 6.5844\n",
      "step 2100: train loss 6.5980\n",
      "step 2200: train loss 6.5728\n",
      "step 2300: train loss 6.5962\n",
      "step 2400: train loss 6.5860\n",
      "step 2500: train loss 6.5841\n",
      "step 2600: train loss 6.6091\n",
      "step 2700: train loss 6.5824\n",
      "step 2800: train loss 6.5992\n",
      "step 2900: train loss 6.6056\n",
      "step 3000: train loss 6.5823\n",
      "step 3100: train loss 6.5923\n",
      "step 3200: train loss 6.5979\n",
      "step 3300: train loss 6.5951\n",
      "step 3400: train loss 6.5856\n",
      "step 3500: train loss 6.5856\n",
      "step 3600: train loss 6.6010\n",
      "step 3700: train loss 6.5808\n",
      "step 3800: train loss 6.5694\n",
      "step 3900: train loss 6.5885\n",
      "step 4000: train loss 6.6017\n",
      "step 4100: train loss 6.5993\n",
      "step 4200: train loss 6.6121\n",
      "step 4300: train loss 6.5708\n",
      "step 4400: train loss 6.5790\n",
      "step 4500: train loss 6.5992\n",
      "step 4600: train loss 6.5856\n",
      "step 4700: train loss 6.5796\n",
      "step 4800: train loss 6.5872\n",
      "step 4900: train loss 6.6050\n",
      "step 4999: train loss 6.5885\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Prepare the data\n",
    "data = torch.tensor(token_ids, dtype=torch.long)\n",
    "train_data = data\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(train_data) - hyperparams['block_size'], (hyperparams['batch_size'],))\n",
    "    x = torch.stack([train_data[i:i+hyperparams['block_size']] for i in ix])\n",
    "    y = torch.stack([train_data[i+1:i+hyperparams['block_size']+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {'train': 0}\n",
    "    model.eval()\n",
    "    losses = torch.zeros(hyperparams['eval_iters'], device=hyperparams['device'])\n",
    "    for k in range(hyperparams['eval_iters']):\n",
    "        X, Y = get_batch()\n",
    "        X, Y = X.to(hyperparams['device']), Y.to(hyperparams['device'])\n",
    "        _, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out['train'] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(model, token_ids, hyperparams):\n",
    "    model.to(hyperparams['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "\n",
    "    for iter in range(hyperparams['max_iters']):\n",
    "        if iter % hyperparams['eval_interval'] == 0 or iter == hyperparams['max_iters'] - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch()\n",
    "        xb, yb = xb.to(hyperparams['device']), yb.to(hyperparams['device'])\n",
    "        _, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Initialize model\n",
    "model = GPT_2(config)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "\n",
    "# start training\n",
    "train_model(model, token_ids, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1385.259065,
   "end_time": "2023-08-10T00:27:21.862849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-10T00:04:16.603784",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
